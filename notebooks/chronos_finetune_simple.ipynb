{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a67776",
   "metadata": {},
   "source": [
    "# Simple Chronos Fine-tuning for INTC Data\n",
    "\n",
    "This notebook:\n",
    "1. Loads INTC 5M stock data\n",
    "2. Fine-tunes a Chronos model\n",
    "3. Saves the model for use in other notebooks\n",
    "\n",
    "**Output**: Fine-tuned model saved to `../models/chronos_finetuned_INTC_5M/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a579d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "264f47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ticker: INTC\n",
      " Timeframe (Frequency): 5T\n",
      " Timeframe (File ID): 5M\n",
      " Base Model: amazon/chronos-bolt-base\n",
      " Prediction Length: 20\n",
      " Model will be saved to: ../models/chronos_finetuned_INTC_5M\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TICKER = \"INTC\"\n",
    "# The pandas frequency string for 5 minutes is '5T'.\n",
    "TIMEFRAME_FREQ = \"5T\"\n",
    "# The identifier for the timeframe in your file names is '5M'.\n",
    "DATA_TIMEFRAME_ID = \"5M\"\n",
    "MODEL_NAME = \"amazon/chronos-bolt-base\" # Using CPU-compatible model\n",
    "PREDICTION_LENGTH = 20\n",
    "\n",
    "# Paths\n",
    "# Construct the path using the correct file name identifier.\n",
    "data_path = f\"../data/{DATA_TIMEFRAME_ID}/{TICKER}_{DATA_TIMEFRAME_ID}.csv\"\n",
    "model_save_dir = f\"../models/chronos_finetuned_{TICKER}_{DATA_TIMEFRAME_ID}\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\" Ticker: {TICKER}\")\n",
    "print(f\" Timeframe (Frequency): {TIMEFRAME_FREQ}\")\n",
    "print(f\" Timeframe (File ID): {DATA_TIMEFRAME_ID}\")\n",
    "print(f\" Base Model: {MODEL_NAME}\")\n",
    "print(f\" Prediction Length: {PREDICTION_LENGTH}\")\n",
    "print(f\" Model will be saved to: {model_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "70087f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Loading data from: ../data/5M/INTC_5M.csv\n",
      "‚úÖ Loaded 11089 rows\n",
      "   Columns: ['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "   Sample datetime values: ['2025-01-23 04:00:00-05:00', '2025-01-23 04:05:00-05:00', '2025-01-23 04:10:00-05:00', '2025-01-23 04:15:00-05:00', '2025-01-23 04:20:00-05:00']\n",
      "‚úÖ Basic datetime conversion successful\n",
      "   ‚ö†Ô∏è Standard conversion failed, trying UTC: Can only use .dt accessor with datetimelike values\n",
      "   UTC conversion successful\n",
      "‚úÖ After sorting and deduplication: 11089 rows\n",
      "‚úÖ Date range: 2025-01-23 09:00:00 to 2025-04-17 23:55:00\n",
      "   Close price range: $17.67 to $27.75\n",
      "   Sample time intervals (minutes): [5.0, 5.0, 5.0, 5.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "print(f\"üìà Loading data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Check data structure\n",
    "    print(f\"   Columns: {df.columns.tolist()}\")\n",
    "    print(f\"   Sample datetime values: {df['Datetime'].head().tolist()}\")\n",
    "    \n",
    "    # Convert datetime - handle timezone issues more carefully\n",
    "    try:\n",
    "        # First try simple conversion\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        print(f\"‚úÖ Basic datetime conversion successful\")\n",
    "        \n",
    "        # Check if timezone aware\n",
    "        if df['Datetime'].dt.tz is not None:\n",
    "            print(f\"   Timezone detected: {df['Datetime'].dt.tz}\")\n",
    "            df['Datetime'] = df['Datetime'].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "            print(f\"   Converted to UTC and removed timezone\")\n",
    "        else:\n",
    "            print(f\"   No timezone detected\")\n",
    "            \n",
    "    except Exception as dt_error:\n",
    "        print(f\"   ‚ö†Ô∏è Standard conversion failed, trying UTC: {dt_error}\")\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True)\n",
    "        df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
    "        print(f\"   UTC conversion successful\")\n",
    "    \n",
    "    # Sort by datetime and remove duplicates\n",
    "    df = df.sort_values('Datetime').drop_duplicates(subset=['Datetime']).reset_index(drop=True)\n",
    "    print(f\"‚úÖ After sorting and deduplication: {len(df)} rows\")\n",
    "    \n",
    "    # Check for data quality\n",
    "    print(f\"‚úÖ Date range: {df['Datetime'].min()} to {df['Datetime'].max()}\")\n",
    "    print(f\"   Close price range: ${df['Close'].min():.2f} to ${df['Close'].max():.2f}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_close = df['Close'].isna().sum()\n",
    "    if missing_close > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Warning: {missing_close} missing Close values - will be dropped\")\n",
    "        df = df.dropna(subset=['Close'])\n",
    "        print(f\"   After dropping missing Close: {len(df)} rows\")\n",
    "    \n",
    "    # Check time intervals (sample first 10)\n",
    "    if len(df) > 1:\n",
    "        time_diffs = df['Datetime'].diff().dt.total_seconds() / 60  # Convert to minutes\n",
    "        print(f\"   Sample time intervals (minutes): {time_diffs.dropna().head(5).tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "03eafbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created TSD with 11089 rows.\n",
      "   Converting to regular frequency '5T'...\n",
      "‚úÖ TimeSeriesDataFrame created with 24372 rows and regular frequency.\n",
      "‚úÖ TimeSeriesDataFrame created with 24372 rows and regular frequency.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Create the required format\n",
    "    ts_df = pd.DataFrame({\n",
    "        'item_id': TICKER,\n",
    "        'timestamp': df['Datetime'],\n",
    "        'target': df['Close']\n",
    "    })\n",
    "    \n",
    "    # Convert to TimeSeriesDataFrame\n",
    "    tsd = TimeSeriesDataFrame.from_data_frame(\n",
    "        ts_df, \n",
    "        id_column='item_id', \n",
    "        timestamp_column='timestamp'\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully created TSD with {len(tsd)} rows.\")\n",
    "\n",
    "    # Convert to a regular frequency. This is the critical step to fix the error.\n",
    "    # It fills gaps (like weekends/holidays) with NaN values so the index is regular.\n",
    "    print(f\"   Converting to regular frequency '{TIMEFRAME_FREQ}'...\")\n",
    "    tsd = tsd.convert_frequency(freq=TIMEFRAME_FREQ)\n",
    "    \n",
    "    if len(tsd) < 100:\n",
    "        print(f\"   ‚ùå Warning: Very few rows ({len(tsd)}) after resampling - this may not be enough for training\")\n",
    "    else:\n",
    "        print(f\"‚úÖ TimeSeriesDataFrame created with {len(tsd)} rows and regular frequency.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating TimeSeriesDataFrame: {e}\")\n",
    "    print(f\"   DataFrame info:\")\n",
    "    print(f\"     Shape: {ts_df.shape}\")\n",
    "    print(f\"     Columns: {ts_df.columns.tolist()}\")\n",
    "    print(f\"     Dtypes: {ts_df.dtypes}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "21751617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data split:\n",
      "   Training: 21934 rows\n",
      "   Validation: 2438 rows\n",
      "   Total: 24372 rows\n"
     ]
    }
   ],
   "source": [
    "# Split data for training (use 90% for training)\n",
    "split_idx = int(len(tsd) * 0.9)\n",
    "train_data = tsd.iloc[:split_idx]\n",
    "val_data = tsd.iloc[split_idx:]\n",
    "\n",
    "print(f\" Data split:\")\n",
    "print(f\"   Training: {len(train_data)} rows\")\n",
    "print(f\"   Validation: {len(val_data)} rows\")\n",
    "print(f\"   Total: {len(tsd)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "81e070bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/chronos_finetuned_INTC_5M\"\n",
      "Frequency '5T' stored as '5min'\n",
      "Beginning AutoGluon training... Time limit = 600s\n",
      "AutoGluon will save models to 'c:\\Users\\micha\\code\\finance\\diploma\\models\\chronos_finetuned_INTC_5M'\n",
      "Frequency '5T' stored as '5min'\n",
      "Beginning AutoGluon training... Time limit = 600s\n",
      "AutoGluon will save models to 'c:\\Users\\micha\\code\\finance\\diploma\\models\\chronos_finetuned_INTC_5M'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          32\n",
      "GPU Count:          0\n",
      "Memory Avail:       36.85 GB / 63.10 GB (58.4%)\n",
      "Disk Space Avail:   128.80 GB / 465.02 GB (27.7%)\n",
      "===================================================\n",
      "Setting presets to: medium_quality\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': MASE,\n",
      " 'freq': '5min',\n",
      " 'hyperparameters': {'Chronos': {'model_path': 'amazon/chronos-bolt-base'}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 20,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 600,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Provided train_data has 21934 rows (NaN fraction=55.3%), 1 time series. Median time series length is 21934 (min=21934, max=21934). \n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.10.18\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          32\n",
      "GPU Count:          0\n",
      "Memory Avail:       36.85 GB / 63.10 GB (58.4%)\n",
      "Disk Space Avail:   128.80 GB / 465.02 GB (27.7%)\n",
      "===================================================\n",
      "Setting presets to: medium_quality\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': MASE,\n",
      " 'freq': '5min',\n",
      " 'hyperparameters': {'Chronos': {'model_path': 'amazon/chronos-bolt-base'}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 20,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 600,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Provided train_data has 21934 rows (NaN fraction=55.3%), 1 time series. Median time series length is 21934 (min=21934, max=21934). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'MASE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'MASE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-02 08:15:57\n",
      "Models that will be trained: ['Chronos[amazon__chronos-bolt-base]']\n",
      "Training timeseries model Chronos[amazon__chronos-bolt-base]. Training for up to 600.0s of the 600.0s of remaining time.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-02 08:15:57\n",
      "Models that will be trained: ['Chronos[amazon__chronos-bolt-base]']\n",
      "Training timeseries model Chronos[amazon__chronos-bolt-base]. Training for up to 600.0s of the 600.0s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Base model: amazon/chronos-bolt-base\n",
      "   Training data: 21934 rows\n",
      "   roughly 5-10 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1141       = Validation score (-MASE)\n",
      "\t0.03    s     = Training runtime\n",
      "\t12.82   s     = Validation (prediction) runtime\n",
      "Not fitting ensemble as only 1 model was trained.\n",
      "Training complete. Models trained: ['Chronos[amazon__chronos-bolt-base]']\n",
      "Total runtime: 12.86 s\n",
      "Best model: Chronos[amazon__chronos-bolt-base]\n",
      "Best model score: -0.1141\n",
      "\t0.03    s     = Training runtime\n",
      "\t12.82   s     = Validation (prediction) runtime\n",
      "Not fitting ensemble as only 1 model was trained.\n",
      "Training complete. Models trained: ['Chronos[amazon__chronos-bolt-base]']\n",
      "Total runtime: 12.86 s\n",
      "Best model: Chronos[amazon__chronos-bolt-base]\n",
      "Best model score: -0.1141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the Chronos model\n",
    "print(f\"   Base model: {MODEL_NAME}\")\n",
    "print(f\"   Training data: {len(train_data)} rows\")\n",
    "print(f\"   roughly 5-10 minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        target='target',\n",
    "        prediction_length=PREDICTION_LENGTH,\n",
    "        path=model_save_dir,\n",
    "        eval_metric='MASE',\n",
    "        freq=TIMEFRAME_FREQ,\n",
    "        verbosity=2\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    predictor.fit(\n",
    "        train_data,\n",
    "        hyperparameters={'Chronos': {\"model_path\": MODEL_NAME}},\n",
    "        presets='medium_quality',\n",
    "        time_limit=600  # 10 min\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Fine-tuning completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    # print(f\"Fine-tuning failed: {e}\")\n",
    "    # print(\"   Troubleshooting info:\")\n",
    "    # print(f\"     Train data shape: {train_data.shape}\")\n",
    "    # print(f\"     Train data index levels: {train_data.index.names}\")\n",
    "    # print(f\"     Sample train data:\")\n",
    "    # if len(train_data) > 0:\n",
    "    #     print(train_data.head())\n",
    "\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80633e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Model saved successfully!\n",
      "üìÅ Location: ../models/chronos_finetuned_INTC_5M\n",
      "üìÑ Info file: ../models/chronos_finetuned_INTC_5M\\model_info.json\n",
      "üéØ To load this model in another notebook:\n",
      "   from autogluon.timeseries import TimeSeriesPredictor\n",
      "   predictor = TimeSeriesPredictor.load('../models/chronos_finetuned_INTC_5M')\n"
     ]
    }
   ],
   "source": [
    "# # Save model information\n",
    "# model_info = {\n",
    "#     'ticker': TICKER,\n",
    "#     'timeframe': TIMEFRAME_FREQ,\n",
    "#     'base_model': MODEL_NAME,\n",
    "#     'prediction_length': PREDICTION_LENGTH,\n",
    "#     'training_rows': len(train_data),\n",
    "#     'model_path': model_save_dir\n",
    "# }\n",
    "\n",
    "# # Save as JSON for easy loading\n",
    "# import json\n",
    "# info_path = os.path.join(model_save_dir, 'model_info.json')\n",
    "# with open(info_path, 'w') as f:\n",
    "#     json.dump(model_info, f, indent=2)\n",
    "\n",
    "# print(\"üíæ Model saved successfully!\")\n",
    "# print(f\"üìÅ Location: {model_save_dir}\")\n",
    "# print(f\"üìÑ Info file: {info_path}\")\n",
    "# print(\"üéØ To load this model in another notebook:\")\n",
    "# print(f\"   from autogluon.timeseries import TimeSeriesPredictor\")\n",
    "# print(f\"   predictor = TimeSeriesPredictor.load('{model_save_dir}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "377878a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleaned\n",
      "‚úÖ Fine-tuning notebook completed!\n",
      "üéØ Fine-tuned model ready at: ../models/chronos_finetuned_INTC_5M\n"
     ]
    }
   ],
   "source": [
    "# Clean up memory\n",
    "try:\n",
    "    if 'predictor' in locals():\n",
    "        del predictor\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memory cleaned\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Memory cleanup had issues, but continuing...\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuning notebook completed!\")\n",
    "print(f\"üéØ Fine-tuned model ready at: {model_save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
