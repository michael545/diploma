{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b837b6",
   "metadata": {},
   "source": [
    "# Chronos Model: 5-Minute and 15-Minute Data Predictions\n",
    "\n",
    "This notebook uses the Chronos Bolt model to generate predictions for stock datasets with 5-minute and 15-minute intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\code\\finance\\diploma\\myenv-3-10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from chronos import ChronosBoltPipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChronosBolt model\n",
    "pipeline = ChronosBoltPipeline.from_pretrained(\n",
    "    \"amazon/chronos-bolt-base\",\n",
    "    device_map=\"auto\",  #\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b60458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window: 512 time steps\n"
     ]
    }
   ],
   "source": [
    "# Get the context window size from the model configuration\n",
    "context_length = pipeline.model.config.n_positions\n",
    "print(f\"Context window: {context_length} time steps\")\n",
    "\n",
    "forecast_horizon = 128  # Same as timesfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable plotting\n",
    "def plot_forecast(dates, actual_values, forecast_values, title, save_path=None):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot actual values as a continuous line\n",
    "    plt.plot(dates, actual_values, color='green', label='Actual', marker='o', markersize=2)\n",
    "    \n",
    "    # Plot forecast values with markers\n",
    "    plt.plot(dates, forecast_values, color='red', linestyle='--', marker='o', markersize=2, label='Predicted')\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=10)\n",
    "    plt.ylabel('Price ($)', fontsize=10)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(frameon=True, shadow=True)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ee265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single dataset\n",
    "def process_dataset(file_path, time_interval):\n",
    "    # Extract ticker symbol from file name\n",
    "    ticker = os.path.basename(file_path).split('_')[0]\n",
    "    print(f'Processing {ticker} dataset with {time_interval} interval...')\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    \n",
    "    # Convert to tensor format for Chronos\n",
    "    input_df = pd.DataFrame({\n",
    "        'unique_id': [1] * len(df),\n",
    "        'ds': df['Datetime'], \n",
    "        'y': df['Close']\n",
    "    })\n",
    "    \n",
    "    # convert values to tensor\n",
    "    values_tensor = torch.tensor(input_df['y'].values, dtype=torch.float32)\n",
    "    \n",
    "    # config\n",
    "    max_start = len(values_tensor) - context_length - forecast_horizon\n",
    "    \n",
    "    if max_start < 0:\n",
    "        print(f'Warning: {ticker} dataset too small for forecasting with current window sizes')\n",
    "        return\n",
    "    \n",
    "    # define the starting points for each backtesting window\n",
    "    backtest_starts = list(range(0, max_start + 1, forecast_horizon))\n",
    "    \n",
    "    # create results directory for this ticker if it doesn't exist\n",
    "    ticker_results_dir = os.path.join(os.getcwd(), \"..\", \"results\", f'chronos_{time_interval}_{ticker}')\n",
    "    os.makedirs(ticker_results_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through each backtesting window\n",
    "    for idx, start_idx in enumerate(backtest_starts):\n",
    "        print(f'Processing window {idx+1}/{len(backtest_starts)}...')\n",
    "        \n",
    "        # Extract context window for this backtesting iteration\n",
    "        context_end = start_idx + context_length\n",
    "        context_window = values_tensor[start_idx:context_end]\n",
    "        \n",
    "        # Make prediction for the forecast horizon\n",
    "        try:\n",
    "            quantiles, mean_forecast = pipeline.predict_quantiles(\n",
    "                context=context_window,\n",
    "                prediction_length=forecast_horizon,\n",
    "                quantile_levels=[0.1, 0.5, 0.9],\n",
    "            )\n",
    "            \n",
    "            # Convert predictions to numpy for easier handling\n",
    "            forecast_values = mean_forecast.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Get actual values for this forecast window\n",
    "            actual_start = context_end\n",
    "            actual_end = actual_start + forecast_horizon\n",
    "            actual_values = values_tensor[actual_start:actual_end].cpu().numpy()\n",
    "            \n",
    "            # Get the corresponding dates\n",
    "            forecast_dates = input_df['ds'].iloc[actual_start:actual_end]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(actual_values, forecast_values)\n",
    "            mse = mean_squared_error(actual_values, forecast_values)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Plot the forecast\n",
    "            title = f'{ticker} ({time_interval}) - Window {idx+1} Forecast (MAE: {mae:.4f}, RMSE: {rmse:.4f})'\n",
    "            plot_path = os.path.join(ticker_results_dir, f'{ticker}_window_{idx+1}.png')\n",
    "            plot_forecast(forecast_dates, actual_values, forecast_values, title, plot_path)\n",
    "            \n",
    "            # Save the results to CSV\n",
    "            results_df = pd.DataFrame({\n",
    "                'date': forecast_dates,\n",
    "                'actual': actual_values,\n",
    "                'forecast': forecast_values\n",
    "            })\n",
    "            csv_path = os.path.join(ticker_results_dir, f'{ticker}_window_{idx+1}.csv')\n",
    "            results_df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            print(f'  MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error processing window {idx+1}: {str(e)}')\n",
    "    \n",
    "    print(f'Finished processing {ticker} dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439320c",
   "metadata": {},
   "source": [
    "## Process 5-Minute Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c716d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\micha\\\\code\\\\finance\\\\diploma\\\\notebooks\\\\data\\\\5M'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data_dir_5m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5M\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get list of all CSV files in the 5M directory\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m csv_files_5m \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir_5m, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir_5m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m csv_files_5m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo CSV files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir_5m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\micha\\\\code\\\\finance\\\\diploma\\\\notebooks\\\\data\\\\5M'"
     ]
    }
   ],
   "source": [
    "# Define 5M data directory\n",
    "data_dir_5m = os.path.join(os.getcwd(), \"..\", \"data\", \"5M\")\n",
    "\n",
    "# Get list of all CSV files in the 5M directory\n",
    "csv_files_5m = [os.path.join(data_dir_5m, f) for f in os.listdir(data_dir_5m) if f.endswith('.csv')]\n",
    "\n",
    "if not csv_files_5m:\n",
    "    print(f\"No CSV files found in {data_dir_5m}\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files_5m)} CSV files in 5M directory:\")\n",
    "    for file in csv_files_5m:\n",
    "        print(f\"- {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69a4b5",
   "metadata": {},
   "source": [
    "## Process 15-Minute Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 CSV files in 15M directory:\n",
      "- INTC_15M.csv\n",
      "- IONQ_15M.csv\n",
      "- MSTR_15M.csv\n",
      "- MU_15M.csv\n",
      "- NVDA_15M.csv\n",
      "- QBTS_15M.csv\n",
      "- RGTI_15M.csv\n",
      "- SMCI_15M.csv\n",
      "- SRPT_15M.csv\n",
      "- TSLA_15M.csv\n",
      "- VKTX_15M.csv\n"
     ]
    }
   ],
   "source": [
    "# Define 15M data directory\n",
    "data_dir_15m = os.path.join(os.getcwd(), \"..\", \"data\", \"15M\")\n",
    "\n",
    "# Get list of all CSV files in the 15M directory\n",
    "csv_files_15m = [os.path.join(data_dir_15m, f) for f in os.listdir(data_dir_15m) if f.endswith('.csv')]\n",
    "\n",
    "if not csv_files_15m:\n",
    "    print(f\"No CSV files found in {data_dir_15m}\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files_15m)} CSV files in 15M directory:\")\n",
    "    for file in csv_files_15m:\n",
    "        print(f\"- {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810744c8",
   "metadata": {},
   "source": [
    "## Process All Tickers (Except NVDA) for Both Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5-minute data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_dir_5m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m             process_dataset(file, interval_name)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing 5-minute data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m process_all_tickers(\u001b[43mdata_dir_5m\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5M\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing 15-minute data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m process_all_tickers(data_dir_15m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15M\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir_5m' is not defined"
     ]
    }
   ],
   "source": [
    "# Process all tickers except NVDA for both intervals\n",
    "def process_all_tickers(interval_dir, interval_name):\n",
    "    csv_files = [os.path.join(interval_dir, f) for f in os.listdir(interval_dir) if f.endswith('.csv')]\n",
    "    for file in csv_files:\n",
    "        if 'NVDA' not in file:\n",
    "            print(f'\\nProcessing {os.path.basename(file)}...')\n",
    "            process_dataset(file, interval_name)\n",
    "\n",
    "print('Processing 5-minute data...')\n",
    "process_all_tickers(data_dir_5m, '5M')\n",
    "\n",
    "print('\\nProcessing 15-minute data...')\n",
    "process_all_tickers(data_dir_15m, '15M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare NVDA across different time intervals\n",
    "# Note: This will only work after you've run predictions for all intervals\n",
    "# compare_ticker_across_intervals('NVDA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-3-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
